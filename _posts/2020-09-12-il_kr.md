---
layout: post
mathjax: true
title: Introduction to Imitation Learning
tags: [Basic, Imitation learning]
ref: 2020-09-12-il
lang: kr
excerpt_separator: <!--more-->
---

## What? Imitation learning (모방학습) 이란?

우리가 일반적으로 기계학습을 통해서 원하는 것은 로봇과 같은 기계가 우리가 원하는 작업을 수행할 수 있도록 학습을 시키는 것입니다. 즉, 학습의 대상인 에이전트(agent; 기계, 로봇 등)가 특정 작업을 수행하는 정책(policy)을 학습하도록 하는 것입니다. 이런 목적으로 사용할 수 있는 학습 기법 중 imitation learning (모방학습)에 대해서 알아보려고 합니다.
<!--more-->

교육심리학에서도 사용되는 imitation learning이라는 용어는 실제로 아이들이 학습하는 방법 중에서 언어 또는 다양한 사회적 행동을 부모나 다른 사람의 행동을 모방함으로써 습득하게 되는 과정을 의미합니다.[^il_human] 이와 마찬가지로 기계 학습 분야에서도 agent가 전문가(expert)의 행동을 모방하는 방식으로 원하는 작업을 수행하도록 학습하시키는 기법을 imitation learning 이라고 합니다.

Expert의 행동을 보고 모방한다는 것은 아직 너무 추상적이기 때문에 Markov Decision Process (MDP) 라는 모델을 간단히 설명하고 이를 기반으로 다시 한 번 이야기 해보겠습니다[^mdp]. 우리가 학습 시키고자 하는 대상인 agent는 environment(환경)이라고 불리는 시스템 내부에 존재하고 있습니다. 이 환경 속에서 agent는 매 순간 처한 state(상황)에 맞는 action(행동)을 취하게 되고, 그 행동의 결과로 agent는 reward(보상)을 얻게되며, 새로운 state에 처하게 됩니다. 여기서 우리의 목표는 agent의 policy(정책), 즉 agent가 주어진 상황에서 어떤 action을 취할 지를 학습하는 것이 목표입니다.

Imitation learning에서는 agent가 expert의 행동을 모방하는 것이 목표라고 했습니다. 위의 모델에서 expert의 행동이라는 것은 특정 state에서 취한 action이 되는 것이고, 이를 모방한다는 것은 agent가 expert와 같은 state에 가게 된다면 expert와 같은 action을 취한다는 것이 됩니다. Agent가 어떤 상황에서든 실제 expert가 할 것 같은 action을 취한다면 우리는 agent가 expert를 잘 모방했다고 이야기 할 수 있습니다.

그럼 expert가 어떻게 행동하는지를 정확하게 알면 그것을 따라하는 것은 매우 쉬울 것 같은데 무엇이 문제일까요? 문제는 agent가 처한 문제 상황은 매우 복잡한 경우가 많은 것에 비해 우리가 expert에거 얻을 수 있는 데이터는 제한적인 상황이 많다는 점입니다.
Imitation learning 문제에서 모방의 대상인 expert는 다양한 형태가 될 수 있습니다. 예를 들어 expert가 특정 상황들에서 취한 action을 모아둔 data (demonstration)가 주어진다거나 또는 특정상황에서 어떤 행동을 취할지 물어보면 대답해주는 형태(query) 등으로 주어지게 됩니다. 하지만 expert가 agent가 처할 수 있는 모든 상황에 대한 데이터 또는 대답을 제공해주지는 못합니다. 따라서 imitation learning에서 중요한 것은 실제로 expert가 보여준(알려준) 적이 없는 상황에서도 expert가 했을 것 같은 행동을 잘 예측해서 agent가 하도록 만드는 것이라고 할 수 있습니다.



## Why? Imitation learning을 왜 사용하는가?

앞에서도 이야기 했듯이 우리가 원하는 것은 agent가 특정 작업을 잘 수행하도록 학습시키는 것입니다. 이를 위한 다양한 학습방법들이 존재 할 수 있을텐데, 그럼 우리는 왜 imitation learning을 사용하는 것일까요? 그리고 모든 학습 방법들이 그러하듯이 imitation learning이 모든 상황에서 사용될 수 있는 것은 아닐텐데, 그럼 어떤 상황에서 잘 사용될수 있을까요? 이해를 돕기위해 비슷한 상황에서 사용될 수 있는 강화학습과의 간단한 비교를 통해 대답해보려고 합니다.

첫 번째로는 주어진 문제에서 reward가 분명하게 정의되지 않는 경우가 많이 있습니다. Agent가 우리가 원하는 작업을 잘 수행하도록 학습을 시킨다고 했는데, 어떤 일을 "잘" 한다는 것을 정의하기 힘들거나 이를 기계인 agent에게 명확하게 전달하기 힘든 경우가 있을 수 있습니다. 간단한 예로 자동차 운전을 생각해볼 수 있습니다. 운전을 잘 한다는 것은 어떻게 정의할 수 있을까요? '빨리가는 것', '안전하게 운전하는 것' 등 다양한 요소들이 섞여있어 정의하기도 힘들지만 정의했다고 해도 하나의 cost 함수로 표현하여 agent에게 전달하는 것은 매우 어려운 일입니다. 대신에 그냥 운전을 잘 하는 사람이 하는 운전을 여러 번 보여주고 이렇게 해달라고 agent에게 부탁하는 것은 상대적으로 쉬울 수 있습니다.

두 번째로 reward가 잘 정의된 상황이라고 하더라도 상황에따라 훨씬 효율적인 학습이 가능합니다. 강화학습을 한번 생각해보면 reward를 최대화 하는 policy를 찾기위해 exploration을 해야합니다. 그리고 주어진 문제 상황에 따라 다르겠지만, 많은 경우 exploration은 매우 오래걸리고 어려운 작업입니다. 이와 같이 어떻게 행동해야 할지 전혀 모르는 상황에서 expert의 행동을 알고 있다면 훨씬 효율적으로 좋은 policy를 찾을 수 있습니다.

Imitation learning은 특정한 일을 잘 수행할 수 있는 expert가 존재하는 경우에만 사용할 수 있는 제한적인 방법입니다. 하지만 반대로 생각해보면 만약 매우 쉽고 저렴하게 expert의 demonstration을 구할 수 있는 상황이라면? 이 경우에 imitation learning은 좋은 방법이 될 수 있습니다.


## How? 어떻게 하는가?

그럼 imitation learning은 어떻게 하는 것일까요? 어떻게 하면 agent가 expert와 비슷하게 행동하게 학습시킬 수 있을까요? 다양한 형태의 학습방법이 제안되었고 최근에도 계속 연구가 진행되고 있습니다. 진행되는 연구들을 크게 두 가지 흐름으로 나누어 볼 수 있는데, 각 방법의 기본적인 아이디어와 한계점에 대해 간단하게 이야기 해보려고 합니다.

#### Behavior cloning
사실 이 문제는 우리에게 익숙한 supervise learning으로 바라볼 수 있습니다. Expert의 demonstration이라는 것은 특정 state (x) 에서 어떤 action (y)를 취할 것인지에 대한 데이터이고, 결국 우리가 하고싶은 것은 이 데이터로부터 expert가 특정 state (x) 에서 어떤 action (y)을 취할지 예측하는 모델을 만드는 것입니다. 이렇게 바라보면 주어진 입력 x에 대한 라벨 y 를 예측하는 간단한 supervise learning 문제로 볼 수 있습니다. 이런 접근 방식을 behavior cloning 이라고 이야기 합니다.[^bc]

그럼 supervise learning에서 사용되는 기법들을 그냥 사용하면 될 것 같은데, 어떤 문제가 있길래 imitation learning을 따로 더 연구하는 것일까요? Supervise learning 의 기본 가정은 training data와 test data가 같은 확률 분포를 갖는다는 것입니다. 따라서 우리가 가지고 있는 training data로 학습한 모델이 실제로도 잘 동작할 수 있는 것입니다.

**Covariate shift in imitation learning.**
Agent가 맞이하는 state가 전문가의 demonstration에 포함되어있지 않았을 수 있습니다. 이런 경우에도 학습한 모델이 적절한 action을 알려줄테지만 최적의 action과는 조금 다를 수 있습니다. Test를 진행할 때, training dataset에 있는 입력만 들어오는 것이 아니라는 점에서 기존의 supervise learning와 크게 다르지 않습니다. 

하지만 일반적으로 imitation learning에서는 연속적인 의사 결정 문제를 다루게 되고, 초반의 작은 에러들이 누적되면 시간이 갈수록 expert의 경험과는 점점 다른 state를 만나게 되고, 결국 expert가 경험한 것과는 전혀 다른 state에서 예상치못한 행동을 하게 될 수 있습니다.

물론 문제의 복잡도에비해 매우 많은 expert demonstration 데이터를 갖고 있다면 전혀 문제될 일이 아니겠지만, 대부분의 경우 우리가 가질 수 있는 데이터는 제한적이기 때문에 효율적인 다른 학습방법들이 필요합니다.

이를 개선하기 위한 다양한 연구들이 진행되고 있으며, 대표적으로 DAgger[^dagger] 와 같은 연구에서는 전문가에게 물어볼 수 있는 환경에서 전문가의 데이터를 효율적으로 모으는 방법에 대해 연구합니다.

#### Inverse Reinforcement Learning
두번째 접근 방법은 inverse reinforcement learning 입니다. 사실 우리의 목표는 expert처럼 동작하도록 agent를 학습시키는 것입니다. 만약 우리가 전문가가 왜 그렇게 행동했는지 그 이유를 알 수 있다면, agent가 주어진 상황에서 expert라면 이런 이유로 이렇게 행동했을 거라는 것을 유추할 수 있을 것입니다. 그럼 전문가가 직접 경험해보지 못했던 상황이 오더라도 agent가 잘 유추해서 적절한 행동할 수 있을 것입니다. 이와 같이 전문가가 어떤 이유로 행동하였는지, 즉 전문가의 policy는 어떤 reward/objective를 최대화 하기위한 것인지 그 reward를 학습하는 방법을 inverse reinforcement learning 이라고 합니다. 이렇게 reward를 알게되면 강화학습을 통해서 우리가 원하는 방향으로 agent를 학습 시킬 수 있을 것입니다.

하지만 이 방법은 expert의 reward를 학습하는 것과 이를 기반으로 한 agent의 policy를 학습하는 두 가지 과정을 반복적으로 수행해야하는데 이 과정이 매우 복잡하고 오래 걸리는 작업입니다. 따라시 이를 효율적으로 하기위한 다양한 방법들이 연구되고 있습니다.

## 정리
imitation learning에 대한 간단한 소개를 해보았습니다. 최근에도 계속 imitation learning에 관한 다양한 연구들이 활발하게 진행되고 있습니다. 특히 Adverserial learning 기법을 적용한 GAIL[^gail], AIRL[^airl] 등의 연구 및 이런 연구의 확장 또한 활발하게 이루어지고 있습니다. 앞으로는 imitation learning의 구체적인 기법에 대해 조금 더 자세히 다루어 보려고 합니다.


-----------------------

[^il_human]: 교육심리학 용어사전: 모방학습 [link](https://terms.naver.com/entry.nhn?docId=1943814&cid=41989&categoryId=41989)
[^mdp]: Markov Decision Process 참고 [link](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwi4ke7phuXoAhUSZt4KHVHeBrgQFjAAegQIARAB&url=https%3A%2F%2Fko.wikipedia.org%2Fwiki%2F%25EB%25A7%2588%25EB%25A5%25B4%25EC%25BD%2594%25ED%2594%2584_%25EA%25B2%25B0%25EC%25A0%2595_%25EA%25B3%25BC%25EC%25A0%2595&usg=AOvVaw1ni1FBGf7V3xVMtxJhIVMB)

[^bc]: Pomerleau, Dean A. "Efficient training of artificial neural networks for autonomous navigation." *Neural computation*, 1991

[^dagger]: Ross, Stéphane, Geoffrey Gordon, and Drew Bagnell. "A reduction of imitation learning and structured prediction to no-regret online learning." *Proceedings of international conference on artificial intelligence and statistics*, 2011. [link](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)

[^airl]: Fu, Justin, Katie Luo, and Sergey Levine. "Learning robust rewards with adversarial inverse reinforcement learning." arXiv preprint arXiv:1710.11248, 2017.

[^gail]: Ho, Jonathan, and Stefano Ermon. "Generative adversarial imitation learning." *Advances in neural information processing systems*, 2016.