---
layout: post
mathjax: true
title: Introduction to Imitation Learning
tags: [Basic, Imitation learning]
ref: 2020-09-12-il
lang: kr
excerpt_separator: <!--more-->
---

## What? Imitation learning (모방학습) 이란?

우리가 일반적으로 기계학습을 통해서 원하는 것은 로봇과 같은 기계가 우리가 원하는 작업을 수행할 수 있도록 학습을 시키는 것입니다. 즉, 학습의 대상인 에이전트(agent; 기계, 로봇 등)가 특정 작업을 수행하는 정책(policy)을 학습하도록 하는 것입니다. 이런 목적으로 사용할 수 있는 학습 기법 중 imitation learning (모방학습)에 대해서 알아보려고 합니다.
<!--more-->

교육심리학에서도 사용되는 imitation learning이라는 용어는 실제로 아이들이 학습하는 방법 중에서 언어 또는 다양한 사회적 행동을 부모나 다른 사람의 행동을 모방함으로써 습득하게 되는 과정을 의미합니다.[^il_human] 이와 마찬가지로 기계 학습 분야에서도 agent가 전문가(expert)의 행동을 모방하는 방식으로 원하는 작업을 수행하도록 학습하시키는 기법을 imitation learning 이라고 합니다.

Expert의 행동을 보고 모방한다는 것은 아직 너무 추상적이기 때문에 Markov Decision Process (MDP) 라는 모델을 간단히 설명하고 이를 기반으로 다시 한 번 이야기 해보겠습니다[^mdp]. 우리가 학습 시키고자 하는 대상인 agent는 environment(환경)이라고 불리는 시스템 내부에 존재하고 있습니다. 이 환경 속에서 agent는 매 순간 처한 state(상황)에 맞는 action(행동)을 취하게 되고, 그 행동의 결과로 agent는 reward(보상)을 얻게되며, 새로운 state에 처하게 됩니다. 여기서 우리의 목표는 agent의 policy(정책), 즉 agent가 주어진 상황에서 어떤 action을 취할 지를 학습하는 것이 목표입니다.

Imitation learning에서는 agent가 expert의 행동을 모방하는 것이 목표라고 했습니다. 위의 모델에서 expert의 행동이라는 것은 특정 state에서 취한 action이 되는 것이고, 이를 모방한다는 것은 agent가 expert와 같은 state에 가게 된다면 expert와 같은 action을 취한다는 것이 됩니다. Agent가 어떤 상황에서든 실제 expert가 할 것 같은 action을 취한다면 우리는 agent가 expert를 잘 모방했다고 이야기 할 수 있습니다.

그럼 expert가 어떻게 행동하는지를 정확하게 알면 그것을 따라하는 것은 매우 쉬울 것 같은데 무엇이 문제일까요? 문제는 agent가 처한 문제 상황은 매우 복잡한 경우가 많은 것에 비해 우리가 expert에거 얻을 수 있는 데이터는 제한적인 상황이 많다는 점입니다.
Imitation learning 문제에서 모방의 대상인 expert는 다양한 형태가 될 수 있습니다. 예를 들어 expert가 특정 상황들에서 취한 action을 모아둔 data (demonstration)가 주어진다거나 또는 특정상황에서 어떤 행동을 취할지 물어보면 대답해주는 형태(query) 등으로 주어지게 됩니다. 하지만 expert가 agent가 처할 수 있는 모든 상황에 대한 데이터 또는 대답을 제공해주지는 못합니다. 따라서 imitation learning에서 중요한 것은 실제로 expert가 보여준(알려준) 적이 없는 상황에서도 expert가 했을 것 같은 행동을 잘 예측해서 agent가 하도록 만드는 것이라고 할 수 있습니다.



## Why? Imitation learning을 왜 사용하는가?

앞에서도 이야기 했듯이 우리가 원하는 것은 agent가 특정 작업을 잘 수행하도록 학습시키는 것입니다. 이를 위한 다양한 학습방법들이 존재 할 수 있을텐데, 그럼 우리는 왜 imitation learning을 사용하는 것일까요? 그리고 모든 학습 방법들이 그러하듯이 imitation learning이 모든 상황에서 사용될 수 있는 것은 아닐텐데, 그럼 어떤 상황에서 잘 사용될수 있을까요? 이해를 돕기위해 비슷한 상황에서 사용될 수 있는 강화학습과의 간단한 비교를 통해 대답해보려고 합니다.

첫 번째로는 주어진 문제에서 reward가 분명하게 정의되지 않는 경우가 많이 있습니다. Agent가 우리가 원하는 작업을 잘 수행하도록 학습을 시킨다고 했는데, 어떤 일을 "잘" 한다는 것을 정의하기 힘들거나 이를 기계인 agent에게 명확하게 전달하기 힘든 경우가 있을 수 있습니다. 간단한 예로 자동차 운전을 생각해볼 수 있습니다. 운전을 잘 한다는 것은 어떻게 정의할 수 있을까요? '빨리가는 것', '안전하게 운전하는 것' 등 다양한 요소들이 섞여있어 정의하기도 힘들지만 정의했다고 해도 하나의 cost 함수로 표현하여 agent에게 전달하는 것은 매우 어려운 일입니다. 대신에 그냥 운전을 잘 하는 사람이 하는 운전을 여러 번 보여주고 이렇게 해달라고 agent에게 부탁하는 것은 상대적으로 쉬울 수 있습니다.

두 번째로 reward가 잘 정의된 상황이라고 하더라도 상황에따라 훨씬 효율적인 학습이 가능합니다. 강화학습을 한번 생각해보면 reward를 최대화 하는 policy를 찾기위해 exploration을 해야합니다. 그리고 주어진 문제 상황에 따라 다르겠지만, 많은 경우 exploration은 매우 오래걸리고 어려운 작업입니다. 이와 같이 어떻게 행동해야 할지 전혀 모르는 상황에서 expert의 행동을 알고 있다면 훨씬 효율적으로 좋은 policy를 찾을 수 있습니다.

Imitation learning은 특정한 일을 잘 수행할 수 있는 expert가 존재하는 경우에만 사용할 수 있는 제한적인 방법입니다. 하지만 반대로 생각해보면 만약 매우 쉽고 저렴하게 expert의 demonstration을 구할 수 있는 상황이라면? 이 경우에 imitation learning은 좋은 방법이 될 수 있습니다.


## How? 어떻게 하는가?

그럼 imitation learning은 어떻게 하는 것일까요? 어떻게 하면 agent가 expert와 비슷하게 행동하게 학습시킬 수 있을까요? 다양한 형태의 학습방법이 제안되었고 최근에도 계속 연구가 진행되고 있습니다. 진행되는 연구들을 크게 두 가지 흐름으로 나누어 볼 수 있는데, 각 방법의 기본적인 아이디어와 한계점에 대해 간단하게 이야기 해보려고 합니다.

#### Behavior cloning
사실 이 문제는 우리에게 익숙한 supervise learning으로 바라볼 수 있습니다. Expert의 demonstration이라는 것은 특정 state (x) 에서 어떤 action (y)를 취할 것인지에 대한 데이터이고, 결국 우리가 하고싶은 것은 이 데이터로부터 expert가 특정 state (x) 에서 어떤 action (y)을 취할지 예측하는 모델을 만드는 것입니다. 이렇게 바라보면 주어진 입력 x에 대한 라벨 y 를 예측하는 간단한 supervise learning 문제로 볼 수 있습니다. 이런 접근 방식을 behavior cloning 이라고 이야기 합니다.[^bc]

그럼 supervise learning에서 사용되는 기법들을 그냥 사용하면 될 것 같은데, 어떤 문제가 있길래 imitation learning을 따로 더 연구하는 것일까요? Supervise learning 의 기본 가정은 training data와 test data가 같은 확률 분포를 갖는다는 것입니다. 따라서 우리가 가지고 있는 training data로 학습한 모델이 실제로도 잘 동작할 수 있는 것입니다.

**Covariate shift in imitation learning.**
Agent가 맞이하는 state가 전문가의 demonstration에 포함되어있지 않았을 수 있습니다. 이런 경우에도 학습한 모델이 적절한 action을 알려줄테지만 최적의 action과는 조금 다를 수 있습니다. Test를 진행할 때, training dataset에 있는 입력만 들어오는 것이 아니라는 점에서 기존의 supervise learning와 크게 다르지 않습니다. 

하지만 일반적으로 imitation learning에서는 연속적인 의사 결정 문제를 다루게 되고, 초반의 작은 에러들이 누적되면 시간이 갈수록 expert의 경험과는 점점 다른 state를 만나게 되고, 결국 expert가 경험한 것과는 전혀 다른 state에서 예상치못한 행동을 하게 될 수 있습니다.

물론 문제의 복잡도에비해 매우 많은 expert demonstration 데이터를 갖고 있다면 전혀 문제될 일이 아니겠지만, 대부분의 경우 우리가 가질 수 있는 데이터는 제한적이기 때문에 효율적인 다른 학습방법들이 필요합니다.

이를 개선하기 위한 다양한 연구들이 진행되고 있으며, 대표적으로 DAgger[^dagger] 와 같은 연구에서는 전문가에게 물어볼 수 있는 환경에서 전문가의 데이터를 효율적으로 모으는 방법에 대해 연구합니다.

#### Inverse Reinforcement Learning
두번째 접근 방법은 inverse reinforcement learning 입니다. 사실 우리의 목표는 expert처럼 동작하도록 agent를 학습시키는 것입니다. 만약 우리가 전문가가 왜 그렇게 행동했는지 그 이유를 알 수 있다면, agent가 주어진 상황에서 expert라면 이런 이유로 이렇게 행동했을 거라는 것을 유추할 수 있을 것입니다. 그럼 전문가가 직접 경험해보지 못했던 상황이 오더라도 agent가 잘 유추해서 적절한 행동할 수 있을 것입니다. 이와 같이 전문가가 어떤 이유로 행동하였는지, 즉 전문가의 policy는 어떤 reward/objective를 최대화 하기위한 것인지 그 reward를 학습하는 방법을 inverse reinforcement learning 이라고 합니다. 이렇게 reward를 알게되면 강화학습을 통해서 우리가 원하는 방향으로 agent를 학습 시킬 수 있을 것입니다.

하지만 이 방법은 expert의 reward를 학습하는 것과 이를 기반으로 한 agent의 policy를 학습하는 두 가지 과정을 반복적으로 수행해야하는데 이 과정이 매우 복잡하고 오래 걸리는 작업입니다. 따라시 이를 효율적으로 하기위한 다양한 방법들이 연구되고 있습니다.

## 정리
imitation learning에 대한 간단한 소개를 해보았습니다. 최근에도 계속 imitation learning에 관한 다양한 연구들이 활발하게 진행되고 있습니다. 특히 Adverserial learning 기법을 적용한 GAIL[^gail], AIRL[^airl] 등의 연구 및 이런 연구의 확장 또한 활발하게 이루어지고 있습니다. 앞으로는 imitation learning의 구체적인 기법에 대해 조금 더 자세히 다루어 보려고 합니다.



< < < **여기까지** > > >




_______________
Agent가 expert(전문가)가 주어진 작업을 수행하는 것을 관측하고 이를 모방하여 주어진 작업을 행동할 수 있도록 agent의 policy(정책)를 학습시키는 방법입니다.
<!--more-->







 하고싶다고 생각해봅시다. Agent가 우리가 원하는대로 행동하도록 학습시키는 여러가지 방법을 생각해볼 수 있습니다. 

그 중에서 imitation learning (모방학습)이라는 방법도 

이와 같은 방법을 기계학습에 적용한 것이라고 할 수 있다.
Agent가 expert(전문가)가 주어진 작업을 수행하는 것을 관측하고 이를 모방하여 주어진 작업을 행동할 수 있도록 agent의 policy(정책)를 학습시키는 방법입니다.
<!--more-->

__________
Agent가 expert(전문가)가 주어진 작업을 수행하는 것을 관측하고 이를 모방하여 주어진 작업을 행동할 수 있도록 agent의 policy(정책)를 학습시키는 방법입니다.
<!--more-->
실제로 아이들이 학습하는 방법 중에서 언어 또는 다양한 사회적 행동을 부모나 다른 사람의 행동을 모방함으로써 습득하게 되는 과정을 모방학습이라고 하는데[^il_human] 이와 같은 방법을 기계학습에 적용한 것이라고 할 수 있다.

전문가의 행동을 따라한다는 개념을 다음과 같이 조금 더 구체화하여 이야기 해볼 수 있다.
Environment(환경)이라고 불리는 시스템 내부에 우리가 학습을 시키고 싶은 대상인 agent가 존재한다. 그리고 환경 속에서 agent는 매 순간 처한 state(상황)에 맞는 action(행동)을 취하게 되고, 그 행동의 결과로 agent는 새로운 state에 처하게 된다. imitation learning에서는 agent가 주어진 상황에서 어떤 action을 취할지, 즉 policy(정책)를 학습하게 된다[^mdp]. 
우리가 모방하고 싶은 대상인 expert 또한 자신의 policy를 가지고 있다고 생각할 수 있고 이에 따라서 주어진 state에서 맞는 action을 취하게 될 것이다. 만약 우리의 agent가 주어진 상황에서 expert와 같은 action을 취한다면 우리는 agent가 expert를 잘 모방했다고 이야기 할 수 있다. 

실제 imitation learning 문제에서 모방의 대상인 expert는 다양한 형태로 주어질 수 있다. 예를 들어 expert가 특정 상황들에서 취한 action을 모아둔 data의 형태 (demonstration) 또는 특정상황에서 어떤 행동을 취할지 물어보면 대답해주는 형태(demonstrator) 등으로 주어질 수 있으며, 이를 바탕으로 모방학습이 이루어지게 된다. 이렇게 전문가의 정보가 주어졌을 때 전문가의 행동을 최대한 모방하여 행동하는 정책을 찾는 것이 imitation learning의 목표이다.

__________________________


<!-- (3~4줄 요약)  
Imitation learning (모방 학습)이란 agent가 expert(전문가)가 주어진 작업을 수행하는 것을 관측하고 모방하여 주어진 작업을 행동할 수 있도록 agent의 policy를 학습시키는 방법이다.-->


IL tries lean directly from demonstration instead of learning by try and error. 
Assumption. human expert can perform the task
obaserve --> get data  --> learn the policy

트라이얼 앤 에러. Exploration 하는 대신에 learn from demonstration 하겠다.

We have the expert. Directly get the policy from 

- Cost can be hard to specify (e.g., driving: multiple objective to combine/trade-offs)
- Good demonstrations are often easier to obtain/provide
- More efficient/practical than learning by Trial-and-error (avoid interactable exploration)
- Can also be used to "distill" a policy approximating a too slow planning precedure


Behaviour Cloning
일반적인 supervised learning과 차이 차이점?
key difference:
Compounding error effect:
slight errors lead to less likly states in training set, where errors will be large/more frequent

Train/Test Mismatch: Breaks supervise learning **i.i.d. assumptions**

- 이게 왜 문제야?
- SUppose experg has a more complex policy than what the model can fit
	- 지금 강의에 나와있는 그림은 train하는 부분이랑 test 하는 부분이 비슷한 분포 또는 locality 를 가지고 있으면 얼추 맞을텐데, 다른 부분에 대한 질문을 하면 틀릴 확률이 높다는걸 그림으로 보여줌
- High complexity model
	- Good fit near examples / poor generalization
- 그냥 오버피팅 언더피팅 이야기 하는거지
- 트레이닝 에러가 테스트 에러가 낮음을 보장하지 못한다
- 근데 그래서 어쩌라고?????

Key difference 2

- Good decisions may require long term reasoning poorly captured in current observation/state
	- Feature capture long-term 
	- Deep learning to implicity long term feature

Category of IL

- Behaviour Cloning
- Interactive Learning / Direct Policy Learning
	- DAGGER
	- SEARN / SMILE
- Inverse Reinforcement Learning
	- GAIL
	- Deep IRL

## Dagger

Intuition: Need to collect more data where learned policy  goes to learn to recover

Iterative interactive approach: Dataset Aggregation (DAGGER)
Expert 가 새로운 데이터를 물어보면 알려줌

--> Execute current policy and Query Expert --> New Data --> Aggregate Dataset --> New policy




## Why? Imitation learning을 왜 사용하는가?

우리가 원하는 것은 agent가 우리가 원하는 일을 잘 수행하도록 학습하는 것이다. 이를 위한 다양한 학습 방법들이 존재할 수 있는데, imitation learning은 특정한 일을 잘 수행할 수 있는 expert가 존재하는 경우에만 사용할 수 있는 제한적인 방법이다. 또한 비슷한 모델에 기반하여 expert의 도움 없이 agent의 policy 를 학습할 수 있는 강화학습이라는 학습방법도 존재한다. 그렇다면 왜 그리고 언제 imitation learning이 사용되는 것일까?

Agent가 우리가 원하는 일을 잘 수행하도록 학습을 시킨다고 했는데, 어떤 일을 "잘" 한다는 것을 정의하기 힘들거나 이를 기계인 agent에게 명확하게 전달하기 힘든 경우가 있을 수 있다. 예를 들어, 운전을 배우는 경우를 생각해보자. 운전을 잘 한다는 것은 어떻게 정의할 수 있을까? 다양한 기준들에 따른 다양한 정의가 있을 수 있다. 예를 들어 '안전하게 운전하는 것', '승객이 편안함을 느끼도록 운전하는 것' 등의 정의가 있을 수 있지만 이들은 너무 추상적이라 이를 목표로 agent를 학습시키기엔 어려움이 있다. 대신 운전을 안전하고, 편안하게 할 수 있는 expert가 존재하여 매 순간 expert라면 어떻게 행동했을지 알 수 있다면, 이를 바탕으로 원하는 행동을 하도록 학습할 수 있을 것이다.


비슷한 모델에 기반하여 expert의 도움 없이 agent의 policy 를 학습할 수 있는 강화학습이라는 학습방법과의 비교를 통하여 왜 imitation learning을 사용하는지 이야기 해 볼수 있다. 앞서 이야기한 바를 다시 이야기한다면 강화학습에서 리워드 자체가 정의되어 있지 않았고 우리가 직접 정의하기 힘든경우가 이에 해당한다. 또한 리워드가 주어지더라도 학습이 쉽지 않은 경우가 있을 수 있다. (예를 들어 search space가 너무 넓은 경우 등).  어떻게 행동해야 할지 전혀 모르는 상황에서 expert의 행동을 알고 있다면 훨씬 효율적으로 학습이 가능해진다. 

## 어떻게 하는가?

그럼 imitation learning은 어떻게 하는 것일까? expert의 demonstration이 주어졌을 때 이를 이용하여 agent를 학습시키는 다양한 방법들이 존재할 수 있다. 그 중 가장 대표적인 두 가지 접근 방법을 소개하려고 한다.

### Behavior cloning

기본적으로 우리가 모방해야 할 것은 '어떤 state가 주어졌을 때 어떤 action을 취할 것인가?' 이다. 

다른 방법 중에는 어떠한 상태에서 어떠한 행동을 할지를 직접 모델링하는 Behavioral Cloning(BC)이라는 것이 있지만, 충분한 data의 양이 필요하고 시간이 지남에 따라 에러가 누적되어 그 누적된 에러 때문에 시간이 지남에 따라 성능이 많이 떨어지게 됩니다. 쉽게 말해 정해진 경로가 있을 때 경로에 조금만 틀어져도 에러가 생기는데 이 에러가 계속 누적되기 때문에 나중에는 크게 달라져버린다는 것입니다. 이러한 단점 때문에 reward function을 모델링하는 IRL 방법이 개발되었습니다. $O(n \log{n})$

### Inverse RL

Inverse RL(IRL)은 expert의 demonstrations (trajectories)가 있을 때 이것을 통해 expert의 optimal policy $\pi$ 를 찾고, 그 policy로 IRL을 진행하여 reward function R 을 찾는 것을 말합니다.




<!--좋은 퀄리티의 전문가가 존재하는 상황에서
- 리워드 자체가 정의하기 힘든경우 (원하는 일을 정의하기 힘든 경우)
- 리워드는 정의할 수 있는데 학습이 힘든 경우 (원하는 일이 정의는 되지만 학습하기가 힘든 경우)-->


_____ 

<!--그럼 우리는 왜 imitation learning을 사용하는 것일까?-->
우리가 원하는 것은 agent가 우리가 원하는 일을 잘 수행하도록 학습하는 것이다. 
하지만 어떤 일을 "잘" 한다는 것을 정의하기 힘든 경우가 있을 수 있으며, 이를 정의할 수 있더라도 목적을 달성하는 policy를 학습하기가 어려운 경우가 있을 수 있다. 이때 만약 그 일을 잘 수행하는 expert가 존재한다면 이를 모방하는 것을 목표로 agent를 학습시킴으로써 우리는 우리가 원하는 일을 agent가 잘 수행하도록 학습 시킬 수 있다.



이를 위해서 우리는 (i) 우리가 원하는 일을 정확하게 정의할 수 있어야하며, (ii)  agent에게 잘 설명할 수 있어야 한다. 

원하는 일이 있는데 그 일을 잘 학습하기 위해 expert를 활용하는 것
우리가 원하는 일 자체가 expert처럼 동작하는 것



언제 사용하는가?
- 좋은 퀄리티의 전문가가 존재하는 상황에서
- 리워드 자체가 정의하기 힘든경우 (원하는 일을 정의하기 힘든 경우)
- 리워드는 정의할 수 있는데 학습이 힘든 경우 (원하는 일이 정의는 되지만 학습하기가 힘든 경우)


처음 운전을 배우는 상황을 예를 들어 생각을 해보자. 
운전을 잘 한다는 것은 어떤 것일까? 우리는 운전할 때 매우 많은것을 고려한다. 예를 들어 어떤 속도로 주행을 할지, 주변차들과의 거리는 어느 정도로 유지할지, 보행자가 있을 때는 어떻게 행동해야할지 등 다양한 요소들이 복합적으로 고려되어 운전을 하게 된다. 따라서 운전을 잘 한다는 것을 정확하게 정의하는 것 자체가 쉽지 않다. Reward를 기반으로 학습하는 강화학습의 관점에서 본다면, reward를 정의하는 것이 쉽지 않은 경우에 해당한다.

그리고 만약 원하는 일을 잘 정의를 할 수 있다고 하더라도 

우리는 agent가 우리가 원하는 일을 잘 수행하도록 학습시키고자 한다. 하지만 우리가 원하는 일 이라는 것을 어떻게 정의하고 설명해야 할까? 처음 운전을 배우는 상황을 예를 들어 생각을 해보자. 운전하는 방법을 말로 어느 정도 설명은 할 수 있겠지만 말로만 완벽하게 전달하는 것은 매우 어려운 일이다. 또한 운전을 잘 하는 것이 어떤 것인지 말로 정의하는 것 자체도 매우 힘든일이다. 자동차의 조작법부터 주행중에 신경써야할 것들, 주변 차와의 거리 커브에서 핸들을 꺽는 정도 등




예를 들어서 로봇 팔이 물체를 원하는 장소로 옮기도록 하고싶다고 해보자. 이런 목표를 컴퓨터에서 인식시켜서 이를 학습시키는 것은 쉽지 않다. 이를 위한 한가지 방법으로는 실제로 물체를 원하는 장소로 옮기는 것을 사람(expert)이 보여주고 로봇이 이를 따라하도록 학습시키는 것을 생각해볼 수 있다. 이렇게 우리가 하고싶은 일이 정확하게 컴퓨터가 이해할 수 있는 형태로 표현하기 힘든 경우, 또는 표현할 수 있더라도 그를 달성할 수 있는 방법이 너무 다양하여 쉽게 학습하기 힘든경우에 imitation lenaring이 사용 될 수 있다.

우리는 agent가 우리가 원하는 일을 잘 수행하도록 학습시키고자 한다. 하지만 우리가 원하는 일 이라는 것을 어떻게 정의하고 설명해야 할까? 예를 들어서 로봇 팔이 물체를 원하는 장소로 옮기도록 하고싶다고 해보자. 이런 목표를 컴퓨터에서 인식시켜서 이를 학습시키는 것은 쉽지 않다. 이를 위한 한가지 방법으로는 실제로 물체를 원하는 장소로 옮기는 것을 사람(expert)이 보여주고 로봇이 이를 따라하도록 학습시키는 것을 생각해볼 수 있다. 이렇게 우리가 하고싶은 일이 정확하게 컴퓨터가 이해할 수 있는 형태로 표현하기 힘든 경우, 또는 표현할 수 있더라도 그를 달성할 수 있는 방법이 너무 다양하여 쉽게 학습하기 힘든경우에 imitation lenaring이 사용 될 수 있다.


- 기계에게 우리가 하고싶은 일을 설명할 방법으로 사용 가능함
- 리워드를 

____


- 모방학습이란 무엇인가?
	- Agent가 expert를 따라하도록 policy 를 학습하는 것이다. 다음에 모델에 대한 설명을 할 기회가 있을 듯. 
		- Imitation Learning in a Nutshell
			- Given: demonstrations or demonstrator 
			- Goal: train a policy to mimic demonstrations
	- 사람도 어릴때 처음 무엇인가를 배울 때 이런 방식으로 학습 한다고 함 (참고자료) [link](https://terms.naver.com/entry.nhn?docId=1943814&cid=41989&categoryId=41989)
	- 전문가가 하는 걸 잘 보고 있다가 따라하면 됨
		- 전문가와 인터렉션 하는 방법에따라 몇가지 방법으로 나뉘어지게 된다.
- 왜 모방학습을 사용하는가?
	- 기계에게 우리가 하고싶은 일을 설명할 방법으로 사용 가능함
	- 리워드를 

- 기본적인 방법
	- BC
		- 이 상황에서는 이렇게 행동하면 되는구나. 그럼 상황이 조금 달라지면 어떻게 함?
	- IRL 이랑 비슷한 예시로서
		- 이걸 하면 저렇게 되는구나. 그럼 이걸 잘 하면 되겠다.

- 한계 및 어려운점
	- (Covariate shift) 그럼 문제는 뭐임? 상황이 조금 달라지면 어떻게 함? 이게 가장 큰 문제야. 이 문제를 해결하기 위해서 다양한 방법들이 제시됨.
	- 모방을 얼마나 잘 했는지에대한 기준
	- 잘못된 인과관계를 학습 할 수 있음

- 강화학습과의 비교
	- 비슷한 점: 모델과 에이전트의 폴리시를 학습한다는 방법이 비슷함
	- 차이점: Objective 가 다름 reward의 존재 유무, exploration
	
__________________________

Imitation learning 구성요소
ICML 2018 Tutorial [link](https://sites.google.com/view/icml2018-imitation-learning/)

- Expert (Demonstration or demonstrator)
- Environment / Simulator
- Policy
- Loss function
- Learning algorithm


위의 정의를 보면 몇 가지 질문이 생긴다.

**Q. 그럼 왜 imitation learning을 사용하는 것일까?**

많은 경우 우리는 agent가 우리가 원하는 일을 잘 수행하도록 하고싶어한다. 하지만 우리가 원하는 일 이라는 것을 어떻게 정의하고 설명해야 할까? 예를 들어서 로봇 팔이 물체를 원하는 장소로 옮기도록 하고싶다고 해보자. 이런 목표를 컴퓨터에서 인식시켜서 이를 학습시키는 것은 쉽지 않다. 이를 위한 한가지 방법으로는 실제로 물체를 원하는 장소로 옮기는 것을 사람(expert)이 보여주고 로봇이 이를 따라하도록 학습시키는 것을 생각해볼 수 있다. 이렇게 우리가 하고싶은 일이 정확하게 컴퓨터가 이해할 수 있는 형태로 표현하기 힘든 경우, 또는 표현할 수 있더라도 그를 달성할 수 있는 방법이 너무 다양하여 쉽게 학습하기 힘든경우에 imitation lenaring이 사용 될 수 있다.

그 예시로는
When teaching a young adult to drive, rather than telling them what the reward function is, it is much easier and more natural to demonstrate driving to them, and have them learn from the demonstration. The task of learning from an expert is called appren- ticeship learning (also learning by watching, imitation learning, or learning from demonstration).

**Q. 강화학습(Reinforcement Learning)과의 관계**

실제로 agent를 우리가 원하는 일을 수행하도록 학습시킨다는 점, 그리고 MDP (Markov decision process)를 이용하여 문제를 모델링하는 방법 등 여러가지 면에서 강화학습과 모방학습은 매우 비슷하다. 가장 큰 차이점은 학습의 목표가 다르다는 점이다. 강화학습에서는 우리가 원하는 목적을 달성하면 높은 reward를 받도록 환경이 설정되어 있고, agent는 그러한 환경 내부에서 reward가 최대화되도록 policy를 학습하는 것이 목표이다. 반면 모방학습에서는 reward가 존재하지 않는 경우가 많으며, expert를 잘 모방하는 것을 목표로 
agent를 학습힌다.

<!-- 예를 들어 deepmind에서 실험했던 atari게임 같은 경우에는 게임의 점수라는 분명한 reward가 존재한다. 하지만 실제 상황에서 우리가 접하는 문제들은 reward가 정의되지 않는 경우가 대부분이다. 앞서 이야기한 로봇 팔의 경우에서도 물체를 정확히 옮기는 일의 경우 정확히 물건을 옮기면 높은 reward를 주어지는 것으로 볼 수도 있지만, 그 이외의 상황에서는 reward를 정의하기 힘들며, 이런경우 agent가 얻게되는 reward가 너무 적어서(sparse) 학습에 어려움을 겪을수도 있다. 이런 상황에서 만약 우리가 원하는 목표를 정확하게 수행할 수 있는 expert가 있어서-->

**Q. 어떻게 동작하는가?**

**Q. Expert를 얼마나 잘 모방했는지 기준이 무엇인가?**





따라서 expert로 부터 

우리는 agent가 우리가 원하는 일을 잘 수행하도록 학습시키고 싶어한다.



우리가 원하는 task가 Agent가 학습하고자하는 목표가

### Expert란 무엇인가?
*Imitation Learning is a sequential task where the learner tries to mimic an expert’s action in order to achieve the best performance.* Global overview of Imitation Learning’, Alexandre, 2018

Imitation Learning (모방 학습) 이란, 다른 Intelligent Agent의 행동을 보고 그것을 따라하면서 학습하는 것이다. 다른 객체의 행동을 따라하고 그에 대한 결과를 관찰하면서, World가 어떻게 구성되어있는지를 이해하며 어떻게 행동해야 할 지 학습해 나간다. RL과 비슷하지만, 다른 객체의 도움을 받으며 학습이 이루어지기 때문에 직접 Exploration을 하지 않아도 된다는 점이 다르다. 즉, 강화학습을 지도학습 쪽으로 변형시킨 것이라고 할 수 있다.
모방학습의 한계 중 하나는, 다른 객체의 행동을 따라하다가 모방해보지 않은 새로운 상황을 마주쳤을 때 어떻게 해야할 지 모른다는 점이다. 이 문제는 모방학습과 강화학습을 결합해서 해결할 수 있을 것이라고 기대한다. 그렇기에 강화학습+모방학습 분야는 아주 전도유망..!


Expert로 부터 무엇을 배워야 하는가?

정책을 배우는 거임


### Access to the reward function: imitation learning or
reinforcement learning.
리워드가 매우 애매한 경우가 많음
- 아타리 게임 같은건 클리어
- 자동차 운전

리워드가 주어진 경우엔 전문가의 데이터가 exploration 을 줄여주는 데 활용 될 수 있음.
둘을 적절이 섞은것도 있음


### Access to system dynamics: model-based or model-free

### Similarity measure between policies. 

In the event that there is not a clear notion of reward function being optimized, a surrogate notion of similarity between the experts’ policy and the learner’s policy needs to be established to reproduce the behavior of the expert. 

This similarity can be defined at the level of individual decisions, although it is usual preferred that the notion of similarity be defined over trajectories the learner and system take together [Ziebart et al., 2013].


Test [^name]

### Behavior cloning
가장 기본적인 방법
Inverse RL(IRL)은 expert의 demonstrations(trajectories)가 있을 때 이것을 통해 expert의 optimal policy π 를 찾고, 그 policy로 IRL을 진행하여 reward function R 을 찾는 것을 말합니다. 다른 방법 중에는 어떠한 상태에서 어떠한 행동을 할지를 직접 모델링하는 Behavioral Cloning(BC)이라는 것이 있지만, 충분한 data의 양이 필요하고 시간이 지남에 따라 에러가 누적되어 그 누적된 에러 때문에 시간이 지남에 따라 성능이 많이 떨어지게 됩니다. 쉽게 말해 정해진 경로가 있을 때 경로에 조금만 틀어져도 에러가 생기는데 이 에러가 계속 누적되기 때문에 나중에는 크게 달라져버린다는 것입니다. 이러한 단점 때문에 reward function을 모델링하는 IRL 방법이 개발되었습니다. $O(n \log{n})$

### Inverse RL

[^il_human]: 교육심리학 용어사전: 모방학습 [link](https://terms.naver.com/entry.nhn?docId=1943814&cid=41989&categoryId=41989)
[^mdp]: Markov Decision Process 참고 [link](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwi4ke7phuXoAhUSZt4KHVHeBrgQFjAAegQIARAB&url=https%3A%2F%2Fko.wikipedia.org%2Fwiki%2F%25EB%25A7%2588%25EB%25A5%25B4%25EC%25BD%2594%25ED%2594%2584_%25EA%25B2%25B0%25EC%25A0%2595_%25EA%25B3%25BC%25EC%25A0%2595&usg=AOvVaw1ni1FBGf7V3xVMtxJhIVMB)

[^bc]: Pomerleau, Dean A. "Efficient training of artificial neural networks for autonomous navigation." *Neural computation*, 1991

[^dagger]: Ross, Stéphane, Geoffrey Gordon, and Drew Bagnell. "A reduction of imitation learning and structured prediction to no-regret online learning." *Proceedings of international conference on artificial intelligence and statistics*, 2011. [link](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)

[^airl]: Fu, Justin, Katie Luo, and Sergey Levine. "Learning robust rewards with adversarial inverse reinforcement learning." arXiv preprint arXiv:1710.11248, 2017.

[^gail]: Ho, Jonathan, and Stefano Ermon. "Generative adversarial imitation learning." *Advances in neural information processing systems*, 2016.